# ðŸ™ï¸ Chicago 311 Service Request Intelligence Platform

> **A Production-Grade ML Portfolio Project** demonstrating end-to-end machine learning engineering on **Azure Databricks** with Lakeflow Declarative Pipelines, SCD Type 2, and MLflow.

[![Azure](https://img.shields.io/badge/Azure-Cloud-0078D4?logo=microsoftazure)](https://azure.microsoft.com)
[![Databricks](https://img.shields.io/badge/Databricks-Sandbox-FF3621?logo=databricks)](https://databricks.com)
[![MLflow](https://img.shields.io/badge/MLflow-Experiment%20Tracking-0194E2?logo=mlflow)](https://mlflow.org)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python)](https://python.org)

---

## ðŸ“‹ Table of Contents

1. [Project Overview](#-project-overview)
2. [ML Portfolio Framework Alignment](#-ml-portfolio-framework-alignment)
3. [Architecture](#-architecture)
4. [Quick Start](#-quick-start)
5. [Component Deep Dives](#-component-deep-dives)
6. [Project Structure](#-project-structure)
7. [Success Metrics](#-success-metrics)
8. [Cost Analysis](#-cost-analysis)

---

## ðŸŽ¯ Project Overview

### Problem Statement

Chicago's 311 service handles thousands of non-emergency requests daily. Operations teams lack proactive tools to detect demand spikes before they overwhelm resources, resulting in degraded response times and inefficient staffing.

### Solution

An end-to-end ML platform that:
- **Forecasts** 7-day service request volumes for staffing optimization
- **Detects anomalies** 2+ hours before spikes overwhelm resources
- **Tracks request lifecycles** via SCD Type 2 for time-in-status analytics
- **Validates data quality** at every pipeline stage with Great Expectations

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA INGESTION                              â”‚
â”‚  Chicago 311 API â†’ API Client â†’ Azure Databricks Volume         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATA QUALITY (Great Expectations)              â”‚
â”‚  Bronze Expectations â†’ Silver Expectations â†’ Gold Expectations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LAKEFLOW PIPELINE (Medallion + SCD2)              â”‚
â”‚  Bronze (Raw) â†’ Silver (SCD2 History) â†’ Gold (Aggregates)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML PIPELINE (MLflow)                         â”‚
â”‚  Feature Engineering â†’ Prophet Training â†’ Anomaly Detection     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SERVING & MONITORING                          â”‚
â”‚  Streamlit Dashboard â† Batch Predictions â† Prediction Logging   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Technical Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Cloud** | Azure + Databricks | Paid sandbox with full feature access |
| **History Tracking** | SCD Type 2 | Enables lifecycle analytics (time-in-status, transitions) |
| **Data Quality** | Great Expectations | Declarative validation with data docs |
| **Pipeline** | Lakeflow Declarative (SQL) | Simplicity, built-in SCD2 via `APPLY CHANGES INTO` |
| **ML Tracking** | MLflow | Native Databricks integration |
| **Forecasting** | Prophet | Handles seasonality, missing data, holidays |
| **Storage** | ADLS Gen2 + Delta Lake | ACID transactions, time travel, Unity Catalog |
| **IaC** | Terraform (azurerm) | Reproducible Azure infrastructure |

### Technology Stack

| Layer | Technology | Why This Choice |
|-------|------------|-----------------|
| **Cloud** | Microsoft Azure | Paid sandbox environment with full capabilities |
| **Compute** | Azure Databricks | Managed Spark, Lakeflow, Unity Catalog |
| **Storage** | ADLS Gen2 + Delta Lake | ACID transactions, time travel, governance |
| **ETL** | Lakeflow Declarative Pipelines | Declarative SQL, auto-optimization |
| **SCD2** | `APPLY CHANGES INTO` | Built-in CDC handling for history tracking |
| **ML Tracking** | MLflow | Native Databricks integration, experiment management |
| **Forecasting** | Prophet | Handles seasonality, missing data, outliers |
| **Dashboard** | Streamlit | Fast prototyping, Python native |
| **IaC** | Terraform (azurerm) | Azure resource provisioning |
| **CI/CD** | GitHub Actions | Free tier, Databricks CLI integration |

---

## ðŸ—ï¸ ML Portfolio Framework Alignment

| Framework Element | Implementation | Status |
|-------------------|----------------|--------|
| **Problem Framing** | 311 demand forecasting & anomaly detection | âœ… |
| **Data Sourcing** | Chicago 311 API (Socrata) with incremental loads | âœ… |
| **Data Quality** | Great Expectations at Bronze/Silver/Gold layers | âœ… |
| **Feature Engineering** | Temporal, lag, rolling, calendar features | âœ… |
| **Model Development** | Prophet + MLflow experiment tracking | âœ… |
| **Deployment** | Batch predictions + Streamlit dashboard | âœ… |
| **Monitoring** | Prediction logging, data drift, DQ dashboards | âœ… |
| **Documentation** | Project scoping, architecture decisions, runbooks | âœ… |

---

## ðŸš€ Quick Start

### Prerequisites

- Azure subscription with Databricks sandbox workspace
- Python 3.11+
- Git
- Terraform CLI (for infrastructure provisioning)
- Azure CLI (`az login`)

### Step 1: Clone Repository

```bash
git clone https://github.com/udayjoshi-captech/chi311-ml-platform.git
cd chi311-intelligence-platform
```

### Step 2: Deploy Azure Infrastructure

```bash
cd infrastructure/terraform
az login
terraform init
terraform plan -var="environment=dev"
terraform apply -var="environment=dev"
```

### Step 3: Configure Databricks Workspace

```bash
# Install Databricks CLI
pip install databricks-cli

# Configure with your Azure Databricks workspace URL
databricks configure --token
# Enter your workspace URL: https://adb-XXXXX.azuredatabricks.net
# Enter your PAT token

# Import notebooks
databricks workspace import_dir ./notebooks /Workspace/Users/YOUR_EMAIL/chi311
```

### Step 4: Set Up Unity Catalog Volumes

1. Open `notebooks/01_setup/00_setup_exploration.py` in Databricks
2. Run all cells to create catalog, schemas, and volumes

### Step 5: Run Data Ingestion

1. Open `notebooks/02_ingestion/01_api_to_volume.py`
2. Run to fetch data from Chicago 311 API into Databricks Volume

### Step 6: Create Lakeflow Pipeline

1. Navigate to **Workflows** â†’ **Lakeflow Declarative Pipelines**
2. Click **Create Pipeline**
3. Point to `pipelines/chi311_scd2_pipeline.sql`
4. Configure: **Serverless** or cluster compute, **Development mode**
5. Run the pipeline

### Step 7: Run ML Training

1. Open `notebooks/04_ml/01_forecasting.py`
2. Attach to compute
3. Run all cells â€” MLflow tracks experiments automatically

### Step 8: Launch Dashboard (Local)

```bash
cd app
pip install -r requirements.txt
streamlit run dashboard.py
```

---

## ðŸ“ Project Structure

```
chi311-intelligence-platform/
â”‚
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ SETUP_GUIDE.md                      # Detailed Azure setup instructions
â”œâ”€â”€ databricks.yml                      # Databricks Asset Bundle config
â”œâ”€â”€ requirements.txt                    # Production dependencies
â”œâ”€â”€ requirements-dev.txt                # Development dependencies
â”œâ”€â”€ setup.py                            # Python package config
â”œâ”€â”€ pytest.ini                          # Test configuration
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROJECT_SCOPING.md              # Problem framing & success metrics
â”‚   â”œâ”€â”€ DATA_SOURCING.md                # Data source documentation
â”‚   â”œâ”€â”€ ARCHITECTURE_DECISIONS.md       # ADRs and alternatives considered
â”‚   â”œâ”€â”€ PIPELINE_EXPLAINED.md           # Beginner-friendly pipeline guide
â”‚   â””â”€â”€ COST_ANALYSIS.md               # Azure cost breakdown
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_setup/
â”‚   â”‚   â””â”€â”€ 00_setup_exploration.py     # Catalog, schemas, volumes, EDA
â”‚   â”œâ”€â”€ 02_ingestion/
â”‚   â”‚   â”œâ”€â”€ 01_api_to_volume.py         # Chicago 311 API â†’ Volume
â”‚   â”‚   â””â”€â”€ 02_bronze_autoloader.py     # Autoloader: Volume â†’ Bronze table
â”‚   â”œâ”€â”€ 03_data_quality/
â”‚   â”‚   â””â”€â”€ 01_data_quality_checks.py   # Great Expectations validation
â”‚   â””â”€â”€ 04_ml/
â”‚       â”œâ”€â”€ 01_forecasting.py           # Prophet + MLflow training
â”‚       â””â”€â”€ 02_anomaly_detection.py     # Anomaly detection pipeline
â”‚
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ chi311_scd2_pipeline.sql        # Lakeflow DLT: Silver SCD2 + Gold
â”‚
â”œâ”€â”€ src/chi311/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ api_client.py               # Robust Socrata API client
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py       # Temporal & lag features
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prophet_forecaster.py        # Prophet wrapper with MLflow
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ prediction_logger.py         # Prediction logging & drift
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_api_client.py
â”‚   â”‚   â””â”€â”€ test_feature_engineering.py
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_pipeline_e2e.py
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ dashboard.py                     # Streamlit dashboard
â”‚   â”œâ”€â”€ requirements.txt                 # Dashboard dependencies
â”‚   â””â”€â”€ Dockerfile                       # Container for deployment
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf                      # Azure resources (RG, ADLS, Databricks)
â”‚       â”œâ”€â”€ variables.tf                 # Terraform variables
â”‚       â””â”€â”€ outputs.tf                   # Terraform outputs
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml                       # CI/CD pipeline
```

---

## ðŸ“Š Success Metrics

| Metric Type | Metric | Target | Current |
|-------------|--------|--------|---------|
| **Business** | Anomaly Detection Lead Time | >2 hours before spike | TBD |
| **Business** | False Positive Rate | <15% | TBD |
| **ML** | Forecast MAPE | <15% | TBD |
| **ML** | Anomaly Precision | >0.80 | TBD |
| **Data** | Pipeline Freshness | <4 hours | TBD |
| **Data** | DQ Pass Rate | >95% | TBD |

---

## ðŸ’° Cost Analysis

See [docs/COST_ANALYSIS.md](docs/COST_ANALYSIS.md) for detailed breakdown.

| Component | Estimated Monthly Cost |
|-----------|----------------------|
| Azure Databricks Compute (sandbox) | Included in subscription |
| ADLS Gen2 Storage (~15 GB) | ~$0.30 |
| Azure Monitor / Log Analytics | ~$1.00 |
| Streamlit Dashboard (local) | $0.00 |
| **Total Infrastructure** | **~$1.30/month** |

> **Note**: With a paid Azure Databricks sandbox, compute costs are covered by the subscription. Infrastructure costs are minimal â€” primarily storage and monitoring.

---

## ðŸ”‘ Key Findings from Data Exploration

| Finding | Value | Impact |
|---------|-------|--------|
| Daily service requests | ~3,000 (excl. info calls) | Baseline for forecasting |
| Info calls share | ~40% ("311 INFORMATION ONLY CALL") | Must filter for ML |
| Status values | Open, Completed, Canceled | SCD2 tracking targets |
| Anomaly threshold | 4,851 (mean + 2Ïƒ) | Detection baseline |
| Weekend drop | 35-40% | Seasonality feature |
| Ward 28 dominance | 39% of requests | Info call admin ward |

---

## ðŸ“„ License

This project is for portfolio/educational purposes. Data sourced from [Chicago Open Data Portal](https://data.cityofchicago.org/) under open data license.
